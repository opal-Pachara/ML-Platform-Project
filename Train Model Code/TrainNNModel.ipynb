{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfcFIjIMeefY",
        "outputId": "99c577c2-dfb8-4d49-fe27-cba6d138b79b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60, Loss: 0.8850762781889542\n",
            "Epoch 2/60, Loss: 0.7918576345495556\n",
            "Epoch 3/60, Loss: 0.7623044109862783\n",
            "Epoch 4/60, Loss: 0.7227088105419408\n",
            "Epoch 5/60, Loss: 0.6791782508725706\n",
            "Epoch 6/60, Loss: 0.6543883915828622\n",
            "Epoch 7/60, Loss: 0.6372234897769016\n",
            "Epoch 8/60, Loss: 0.6260948017887448\n",
            "Epoch 9/60, Loss: 0.6177899328262909\n",
            "Epoch 10/60, Loss: 0.6120253850584445\n",
            "Epoch 11/60, Loss: 0.6080542954413787\n",
            "Epoch 12/60, Loss: 0.6049253144989843\n",
            "Epoch 13/60, Loss: 0.602858904392823\n",
            "Epoch 14/60, Loss: 0.6014519328656404\n",
            "Epoch 15/60, Loss: 0.6001349305329116\n",
            "Epoch 16/60, Loss: 0.5990585832492165\n",
            "Epoch 17/60, Loss: 0.5985308079615883\n",
            "Epoch 18/60, Loss: 0.598149137393288\n",
            "Epoch 19/60, Loss: 0.5975649536951728\n",
            "Epoch 20/60, Loss: 0.5982252781805785\n",
            "Epoch 21/60, Loss: 0.5977700791929079\n",
            "Epoch 22/60, Loss: 0.5969278036252312\n",
            "Epoch 23/60, Loss: 0.5968717518060104\n",
            "Epoch 24/60, Loss: 0.5963890481254329\n",
            "Epoch 25/60, Loss: 0.5958492766255917\n",
            "Epoch 26/60, Loss: 0.5954787157152011\n",
            "Epoch 27/60, Loss: 0.5952530419049056\n",
            "Epoch 28/60, Loss: 0.5949886182080144\n",
            "Epoch 29/60, Loss: 0.5955366226642028\n",
            "Epoch 30/60, Loss: 0.5949156670466713\n",
            "Epoch 31/60, Loss: 0.5949597603600958\n",
            "Epoch 32/60, Loss: 0.5947698663110318\n",
            "Epoch 33/60, Loss: 0.5946827853503435\n",
            "Epoch 34/60, Loss: 0.5946119717929674\n",
            "Epoch 35/60, Loss: 0.5946199966513592\n",
            "Epoch 36/60, Loss: 0.5945317627295204\n",
            "Epoch 37/60, Loss: 0.5948685079813003\n",
            "Epoch 38/60, Loss: 0.5941499016855074\n",
            "Epoch 39/60, Loss: 0.5937572652878969\n",
            "Epoch 40/60, Loss: 0.5936375954876776\n",
            "Epoch 41/60, Loss: 0.5935083073118459\n",
            "Epoch 42/60, Loss: 0.5934767225514288\n",
            "Epoch 43/60, Loss: 0.5932665854692459\n",
            "Epoch 44/60, Loss: 0.5931693052468092\n",
            "Epoch 45/60, Loss: 0.5929063347370728\n",
            "Epoch 46/60, Loss: 0.5928801480842674\n",
            "Epoch 47/60, Loss: 0.5935783458792645\n",
            "Epoch 48/60, Loss: 0.5930839186129363\n",
            "Epoch 49/60, Loss: 0.5939292586368063\n",
            "Epoch 50/60, Loss: 0.5930708005376484\n",
            "Epoch 51/60, Loss: 0.5929904112349386\n",
            "Epoch 52/60, Loss: 0.592795395332834\n",
            "Epoch 53/60, Loss: 0.5928598247144533\n",
            "Epoch 54/60, Loss: 0.5926565502000891\n",
            "Epoch 55/60, Loss: 0.5926495891550313\n",
            "Epoch 56/60, Loss: 0.5927171107219613\n",
            "Epoch 57/60, Loss: 0.5925420548604883\n",
            "Epoch 58/60, Loss: 0.5923925189868263\n",
            "Epoch 59/60, Loss: 0.5924206918996313\n",
            "Epoch 60/60, Loss: 0.5924224778361943\n",
            "Accuracy: 72.93765314456847%\n",
            "Model and Vectorizer saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import joblib\n",
        "\n",
        "# อ่านข้อมูลจาก CSV และจัดการข้อมูลที่หายไป\n",
        "df = pd.read_csv('/content/YoutubeCommentsDataSet.csv')\n",
        "df = df.dropna(subset=['Comment'])\n",
        "df['Comment'] = df['Comment'].fillna('').astype(str)\n",
        "\n",
        "# แปลง Labels เป็นตัวเลข\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(['negative', 'neutral', 'positive'])\n",
        "labels = label_encoder.transform(df['Sentiment'].values)\n",
        "\n",
        "# แปลงข้อความเป็นเวกเตอร์ TF-IDF\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X = tfidf.fit_transform(df['Comment'].values).toarray()\n",
        "\n",
        "# แบ่งข้อมูลเป็น train และ test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# แปลงข้อมูลเป็น Tensor\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# สร้าง DataLoader\n",
        "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=32)\n",
        "\n",
        "# สร้าง Neural Network Model\n",
        "class SentimentNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SentimentNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5000, 128)\n",
        "        self.fc2 = nn.Linear(128, 3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "# กำหนด Loss Function และ Optimizer\n",
        "model = SentimentNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ฝึกโมเดล\n",
        "num_epochs = 60\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# ประเมินโมเดล\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {100 * correct / total}%')\n",
        "\n",
        "# บันทึกโมเดลและตัวแปลงข้อความ\n",
        "torch.save(model.state_dict(), 'sentiment_model.pth')\n",
        "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')\n",
        "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "\n",
        "print(\"Model and Vectorizer saved successfully!\")\n"
      ]
    }
  ]
}